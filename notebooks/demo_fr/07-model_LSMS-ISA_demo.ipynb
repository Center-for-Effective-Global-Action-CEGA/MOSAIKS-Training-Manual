{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp1-HnPiYUoh"
      },
      "source": [
        "# Démonstration MOSAIKS : Identifier l'accès à Internet au Togo",
        "",
        "> **Remarque** : si vous souhaitez enregistrer ce bloc-notes avec vos modifications, assurez-vous de cliquer sur « Fichier > Enregistrer une copie dans Drive ». Toutes les modifications seront perdues si vous fermez cet onglet sans enregistrer une copie dans votre Google Drive. Toutes les modifications effectuées avant d'enregistrer une copie dans Drive seront enregistrées dans le bloc-notes après avoir enregistré une copie dans Drive. Les données sont téléchargées depuis Internet et stockées sur le disque temporaire de l'environnement. Ces données seront supprimées une fois la session terminée.",
        "",
        "Ce carnet de notes présente l'application de l'observation multitâche à l'aide d'images satellite et d'éviers de cuisine (MOSAIKS), une approche d'apprentissage automatique utilisant l'imagerie satellite, pour identifier les zones avec et sans accès à Internet au Togo. Ces informations peuvent être cruciales pour comprendre la fracture numérique et éclairer les politiques visant à améliorer la connectivité."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGgU2UYC8mvt"
      },
      "source": [
        "## Méthodologie",
        "",
        "Le processus comprend les étapes suivantes :",
        "",
        "1. **Acquisition des données :** les données d'étiquetage sur l'accès à Internet sont fournies par l'Agence Togo Digital (ATD). Les caractéristiques des images satellite prétraitées sont dérivées à l'aide du cadre MOSAIKS.",
        "1. **Prétraitement des données :** les données d'étiquettes et de fonctionnalités sont prétraitées et nettoyées. Il en résulte un ensemble de données unique avec des étiquettes et des fonctionnalités prêtes à l'emploi.",
        "1. **Formation et évaluation du modèle :** un modèle d'apprentissage automatique, en particulier Ridge Regression avec étalonnage isotonique, est formé sur l'ensemble de données joint pour prédire la probabilité d'accès à Internet. Le modèle est évalué à l'aide de mesures telles que la zone sous la courbe des caractéristiques de fonctionnement du récepteur (ROC AUC).",
        "1. **Visualisation et interprétation :** les résultats du modèle sont visualisés à l'aide de cartes et de graphiques. Les caractéristiques importantes identifiées par le modèle sont analysées pour comprendre les modèles associés à l'accès à Internet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azwGr9_x8bfh"
      },
      "source": [
        "## Installation",
        "",
        "Ce notebook utilise plusieurs bibliothèques Python clés :",
        "",
        "- **Traitement et analyse des données :** « pandas » et « numpy » fournissent des structures de données et des fonctions fondamentales pour la manipulation et l'analyse des données. « geopandas » étend ces capacités pour travailler avec des données géospatiales.",
        "- **Visualisation :** `matplotlib.pyplot` et `seaborn` permettent la création de visualisations statiques et interactives, facilitant l'exploration des données et la présentation des résultats.",
        "- **Apprentissage automatique :** `sklearn` propose une suite complète d'outils pour créer et évaluer des modèles d'apprentissage automatique, y compris des algorithmes tels que la régression Ridge et des méthodes de prétraitement des données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "executionInfo": {
          "elapsed": 9323,
          "status": "ok",
          "timestamp": 1733431016832,
          "user": {
            "displayName": "Cullen D Molitor",
            "userId": "02217726571370409329"
          },
          "user_tz": 480
        },
        "id": "x6eczjTE2k3B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from google.colab import drive\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.calibration import IsotonicRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "from scipy.linalg import LinAlgWarning\n",
        "\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\", category=LinAlgWarning, module=\"sklearn.linear_model._ridge\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the directory name\n",
        "data_dir = \"LSMS-ISA-data\"\n",
        "\n",
        "# Check if the final directory exists\n",
        "if not os.path.exists(data_dir):\n",
        "    # Download only if the zip file doesn't exist\n",
        "    if not os.path.exists(\"Data.zip\"):\n",
        "        !wget https://zenodo.org/records/14040658/files/Data.zip\n",
        "\n",
        "    # Unzip the data\n",
        "    !unzip Data.zip\n",
        "\n",
        "    # Rename the folder\n",
        "    !mv Data {data_dir}\n",
        "\n",
        "    # Remove the zip file\n",
        "    !rm Data.zip\n",
        "\n",
        "# List the files (this will run regardless)\n",
        "!ls -lh {data_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the directory and base URL\n",
        "geo_dir = \"geoBoundaries\"\n",
        "base_url = \"https://github.com/wmgeolab/geoBoundaries/raw/9469f09/releaseData/gbOpen\"\n",
        "\n",
        "# List of country codes\n",
        "countries = [\"ETH\", \"MWI\", \"MLI\", \"NER\", \"NGA\", \"TZA\", \"UGA\"]\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists(geo_dir):\n",
        "    !mkdir {geo_dir}\n",
        "\n",
        "# Download files for each country if they don't exist\n",
        "for country in countries:\n",
        "    filename = f\"geoBoundaries-{country}-ADM2.geojson\"\n",
        "    filepath = os.path.join(geo_dir, filename)\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        !wget {base_url}/{country}/ADM2/{filename} -P {geo_dir}\n",
        "\n",
        "# List the files (this will run regardless)\n",
        "!ls -lh {geo_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !wget https://www.geoboundaries.org/data/geoBoundariesCGAZ-3_0_0/ADM2/simplifyRatio_100/geoBoundariesCGAZ_ADM2.topojson\n",
        "\n",
        "boundaries = gpd.read_file(\"geoBoundariesCGAZ_ADM2.topojson\")\n",
        "mask = boundaries[\"shapeID\"].str[:3].isin(countries)\n",
        "boundaries = boundaries[mask]\n",
        "boundaries = boundaries.rename(\n",
        "    columns={\n",
        "        \"shapeName\": \"ADM2\",\n",
        "        \"shapeGroup\": \"ADM0\",\n",
        "    }\n",
        ")\n",
        "boundaries = boundaries.drop(\n",
        "    columns=[\n",
        "        \"id\",\n",
        "        \"shapeISO\",\n",
        "        \"shapeType\",\n",
        "        \"ADM1_shapeID\",\n",
        "        \"ADM0_shapeID\",\n",
        "        \"ADMHIERARCHY\",\n",
        "    ]\n",
        ")\n",
        "boundaries = boundaries[[\"shapeID\", \"ADM0\", \"ADM2\", \"geometry\"]]\n",
        "boundaries.crs = \"EPSG:4326\"\n",
        "boundaries.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "executionInfo": {
          "elapsed": 27140,
          "status": "ok",
          "timestamp": 1733431067300,
          "user": {
            "displayName": "Cullen D Molitor",
            "userId": "02217726571370409329"
          },
          "user_tz": 480
        },
        "id": "5ITEdHZP3pdC",
        "outputId": "8ed5b405-3832-4d30-b230-785ec65ca1e2"
      },
      "outputs": [],
      "source": [
        "# feats = pd.read_csv(\n",
        "#     os.path.join(\n",
        "#         \"/\",\n",
        "#         \"home\",\n",
        "#         \"emlab\",\n",
        "#         \"data\",\n",
        "#         \"mosaiks-togo\",\n",
        "#         \"API_features\",\n",
        "#         \"ADM_2_regions_RCF_global_dense.csv\",\n",
        "#     )\n",
        "# )\n",
        "# mask = feats['shapeID'].str[:3].isin(countries)\n",
        "# feats = feats[mask]\n",
        "# feats = feats.drop(columns=[\"shapeID.1\"], errors=\"ignore\")\n",
        "# features = boundaries.set_index(\"shapeID\").join(feats.set_index(\"shapeID\"), how=\"left\")\n",
        "# features = features.reset_index()\n",
        "# new_cols = [col for col in features.columns if col != 'geometry'] + ['geometry']\n",
        "# features = features[new_cols]\n",
        "# features.crs = \"EPSG:4326\"\n",
        "# features.to_feather(\"adm2_api_features.feather\")\n",
        "# features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feats2 = gpd.read_feather(\"adm2_api_features.feather\")\n",
        "feats2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_stata(os.path.join(data_dir, \"Plotcrop_dataset.dta\"))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df = pd.read_stata(os.path.join(data_dir, \"Household_dataset.dta\"))\n",
        "df = pd.read_stata(os.path.join(data_dir, \"Plotcrop_dataset.dta\"))\n",
        "# Create a conditions dictionary mapping countries to their desired waves\n",
        "wave_conditions = {\n",
        "    \"Ethiopia\": 4,\n",
        "    \"Malawi\": 4,\n",
        "    \"Mali\": 2,\n",
        "    \"Niger\": 2,\n",
        "    \"Nigeria\": 3,\n",
        "    \"Tanzania\": 5,\n",
        "    \"Uganda\": 7,\n",
        "}\n",
        "\n",
        "# Create the filter using boolean indexing\n",
        "df = df[\n",
        "    df.apply(lambda x: x[\"wave\"] == wave_conditions.get(x[\"country\"], False), axis=1)\n",
        "]\n",
        "\n",
        "gdf = gpd.GeoDataFrame(\n",
        "    df, geometry=gpd.points_from_xy(df.lon_modified, df.lat_modified), crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "gdf = gdf.drop(\n",
        "    columns=[\n",
        "        \"admin_2\",\n",
        "        \"admin_3\",\n",
        "        \"hh_id_merge\",\n",
        "        \"hh_id_obs\",\n",
        "        \"season\",\n",
        "        \"ea_id_merge\",\n",
        "        \"ea_id_obs\",\n",
        "        \"strataid\",\n",
        "        \"geocoords_id\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "gdf[\"admin_2_name\"] = gdf[\"admin_2_name\"].replace(\"\", np.nan)\n",
        "\n",
        "mask_coords = gdf[\"geometry\"].is_empty == False\n",
        "gdf_with_coords = gdf[mask_coords].copy()\n",
        "\n",
        "joined_data = gpd.sjoin(\n",
        "    gdf_with_coords, boundaries[[\"ADM2\", \"geometry\"]], how=\"left\", predicate=\"within\"\n",
        ")\n",
        "\n",
        "gdf[\"admin_2_joined\"] = None\n",
        "\n",
        "gdf.loc[joined_data.index, \"admin_2_joined\"] = joined_data[\"ADM2\"]\n",
        "\n",
        "gdf[\"ADM2\"] = gdf[\"admin_2_name\"].fillna(gdf[\"admin_2_joined\"])\n",
        "\n",
        "gdf = gdf.drop(\n",
        "    columns=[\n",
        "        \"wave\",\n",
        "        \"admin_2_joined\",\n",
        "        \"geometry\",\n",
        "        \"lat_modified\",\n",
        "        \"lon_modified\",\n",
        "        \"admin_1\",\n",
        "        \"admin_1_name\",\n",
        "        \"admin_2_name\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create dictionary of country names to ISO3 codes\n",
        "country_to_iso3 = {\n",
        "    \"Ethiopia\": \"ETH\",\n",
        "    \"Malawi\": \"MWI\",\n",
        "    \"Mali\": \"MLI\",\n",
        "    \"Niger\": \"NER\",\n",
        "    \"Nigeria\": \"NGA\",\n",
        "    \"Tanzania\": \"TZA\",\n",
        "    \"Uganda\": \"UGA\",\n",
        "}\n",
        "\n",
        "# Create new ADM0 column using map\n",
        "gdf[\"ADM0\"] = gdf[\"country\"].map(country_to_iso3)\n",
        "\n",
        "new_cols = [\"country\", \"ADM2\"] + [\n",
        "    col for col in gdf.columns if col not in [\"country\", \"ADM2\"]\n",
        "]\n",
        "gdf = gdf[new_cols]\n",
        "\n",
        "gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdf.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert Yes/No to 1/0\n",
        "binary_cols = [\"urban\", \"hh_electricity_access\"]\n",
        "for col in binary_cols:\n",
        "    gdf[col] = (gdf[col] == \"Yes\").astype(int)\n",
        "\n",
        "# Identify numeric columns to aggregate\n",
        "numeric_cols = gdf.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
        "numeric_cols = [\n",
        "    col for col in numeric_cols if col != \"pw\" and col != \"ADM0\"\n",
        "]  # exclude weight column\n",
        "\n",
        "# Create weighted means by ADM2\n",
        "summary = []\n",
        "for adm2 in gdf[\"ADM2\"].dropna().unique():\n",
        "    subset = gdf[gdf[\"ADM2\"] == adm2]\n",
        "\n",
        "    # Skip if subset is empty\n",
        "    if len(subset) == 0:\n",
        "        continue\n",
        "\n",
        "    country = subset[\"country\"].iloc[0]\n",
        "    # adm0 = subset[\"ADM0\"].iloc[0]\n",
        "\n",
        "    # Calculate weighted means for each numeric column\n",
        "    weighted_means = {\n",
        "        \"country\": country,\n",
        "        # \"ADM0\": adm0,\n",
        "        \"ADM2\": adm2,\n",
        "        \"n_households\": len(subset),\n",
        "    }\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        # Remove NaN values before calculating weighted average\n",
        "        mask = ~subset[col].isna()\n",
        "        if mask.any():  # if there are any non-NaN values\n",
        "            weighted_means[col] = np.average(\n",
        "                subset[col][mask], weights=subset[\"pw\"][mask]\n",
        "            )\n",
        "        else:\n",
        "            weighted_means[col] = np.nan\n",
        "\n",
        "    summary.append(weighted_means)\n",
        "\n",
        "# Convert to DataFrame\n",
        "summary_df = pd.DataFrame(summary)\n",
        "\n",
        "# Reorder columns to match original\n",
        "new_cols = [\"country\", \"ADM2\", \"n_households\"] + [\n",
        "    col for col in numeric_cols if col in summary_df.columns\n",
        "]\n",
        "summary_df = summary_df[new_cols]\n",
        "summary_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_df[\"ADM2\"] = summary_df[\"ADM2\"].str.title()\n",
        "feats2[\"ADM2\"] = feats2[\"ADM2\"].str.title()\n",
        "\n",
        "summary_df[\"ADM2\"] = summary_df[\"ADM2\"].str.replace(\" Rural\", \"\").str.strip()\n",
        "feats2[\"ADM2\"] = feats2[\"ADM2\"].str.replace(\" Rural\", \"\").str.strip()\n",
        "\n",
        "# Create a dictionary of corrections\n",
        "corrections = {\n",
        "    \"Birni N'Konni\": \"Bkonni\",\n",
        "    \"Butiama\": \"Butiam\",\n",
        "    \"Guidan-Roumdji\": \"Guidan Roumji\",\n",
        "    \"Illéla\": \"Illela\",\n",
        "    \"Matamèye\": \"Matameye\",\n",
        "    \"Tchin-Tabaraden\": \"Tchintabaraden\",\n",
        "    \"Tillaberi\": \"Tillabéri\",\n",
        "    \"Maïné-Soroa\": \"Maïné Soroa\",\n",
        "    \"Abia\": \"Abi\",\n",
        "    \"Anambra East\": \"Anambra\",\n",
        "    \"Babati Urban\": \"Babati\",\n",
        "    \"Kigoma Ujiji Urban\": \"Kigoma Urban\",\n",
        "    \"Niamey1\": \"Niamey\",\n",
        "    \"Babati Town\": \"Babati Urban\",\n",
        "    \"Kahama Town\": \"Kahama Township Authority\",\n",
        "    \"Korogwe Urban\": \"Korogwe Township Authority\",\n",
        "    \"Mafinga Town\": \"Mafinga Township Authority\",\n",
        "    \"Masasi Urban\": \"Masasi Township Authority\",\n",
        "    \"Mtwara Mikindani\": \"Mtwara Urban\",\n",
        "    \"Nzega Town\": \"Nzega Township Authority\",\n",
        "    \"Kasulu Town\": \"Kasulu Township Authority\",\n",
        "    \"Chakechake\": \"Chake Chake\",\n",
        "    \"Fct Abuja\": \"Abuja Municipal\",\n",
        "    \"Handeni Mji\": \"Handeni Urban\",\n",
        "    \"Makambako Town\": \"Makambako Township Authority\",\n",
        "    \"Chakechake\": \"Chake Chake\",  # In case of variations without capitalization\n",
        "    \"Masasi  Township Authority\": \"Masasi Township Authority\",\n",
        "}\n",
        "\n",
        "# Apply corrections to both dataframes\n",
        "for old_name, new_name in corrections.items():\n",
        "    summary_df[\"ADM2\"] = summary_df[\"ADM2\"].replace(old_name, new_name)\n",
        "    feats2[\"ADM2\"] = feats2[\"ADM2\"].replace(old_name, new_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get sets of unique values from each dataset\n",
        "set1 = set(summary_df[\"ADM2\"].unique())\n",
        "set2 = set(feats2[\"ADM2\"].unique())\n",
        "\n",
        "# Find values that appear in one dataset but not the other\n",
        "only_in_summary = set1 - set2\n",
        "only_in_feats = set2 - set1\n",
        "\n",
        "print(\"Values only in summary_df:\", sorted(only_in_summary))\n",
        "print(\"Values only in feats2:\", sorted(only_in_feats))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted(only_in_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = summary_df.merge(feats2, on=[\"ADM2\"], how=\"left\")\n",
        "data = data.dropna()\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.calibration import IsotonicRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1733431071451,
          "user": {
            "displayName": "Cullen D Molitor",
            "userId": "02217726571370409329"
          },
          "user_tz": 480
        },
        "id": "xHYW9_MtqvbK"
      },
      "outputs": [],
      "source": [
        "feature_cols = [f\"X_{i}\" for i in range(4000)]\n",
        "# feature_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.columns[3:25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "executionInfo": {
          "elapsed": 5460,
          "status": "ok",
          "timestamp": 1733431076908,
          "user": {
            "displayName": "Cullen D Molitor",
            "userId": "02217726571370409329"
          },
          "user_tz": 480
        },
        "id": "-hhKBLT22H6x"
      },
      "outputs": [],
      "source": [
        "X = data[feature_cols].values\n",
        "y = data[\"nb_plots\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "alphas = np.logspace(-8, 8, base=10, num=17)\n",
        "ridge = RidgeCV(alphas=alphas, scoring=\"r2\", cv=5)\n",
        "\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "y_pred = np.maximum(ridge.predict(X_test), 0)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best alpha: {ridge.alpha_}\")\n",
        "print(f\"Validation R2 performance {ridge.best_score_:0.2f}\")\n",
        "print(f\"Test R2 performance {r2:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "mosaiks",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}